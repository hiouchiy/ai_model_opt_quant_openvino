{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINOを用いたモデルの最適化と量子化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8shXTSsAXqMt"
   },
   "source": [
    "## ０．事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EZaCumrR3UE"
   },
   "source": [
    "### ０－１．学習済みモデルをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3SoHCEWR8iM"
   },
   "outputs": [],
   "source": [
    "!curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1JXPHg4brau1T93z79VNr4VqLeCEx2CcW\" > /dev/null\n",
    "!CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n",
    "!curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1JXPHg4brau1T93z79VNr4VqLeCEx2CcW\" -o drone_Custum_Trained.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXAwYMI5gvem"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4z9w_OESctY"
   },
   "source": [
    "## １．改めてPyTorchで学習済みモデルを推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpYu-I7bSlsa"
   },
   "source": [
    "### １－１．まずは下で動いているCPUを確認\n",
    "Intel Xeonが動いているか確認してください。他メーカーのCPUの場合はこの先のプログラムが正常に動作しない可能性があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7l1sMpAaSaM3"
   },
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdCKWx4PSwl3"
   },
   "source": [
    "### １－２．必要なライブラリーをインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dH9ewpX7SsMc"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import statistics\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "!pip install -q segmentation-models-pytorch\n",
    "!pip install -q torchsummary\n",
    "\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i84zGDdSZ-L3"
   },
   "source": [
    "### １－３．パスの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0G3aHK7S4hA"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'drone_Custum_Trained.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJVqspoYaDL1"
   },
   "source": [
    "### １－４．必要な関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdIg8lR8TZwb"
   },
   "outputs": [],
   "source": [
    "mapping = {(0, 0, 0): 0,\n",
    "           (150, 143, 9): 1,\n",
    "           (192, 128, 32): 1,\n",
    "           (150, 143, 3): 1,\n",
    "            }    \n",
    "\n",
    "def inverse_dict(map):\n",
    "    return {v:k for k, v in map.items() }\n",
    "\n",
    "reverse_mapping = inverse_dict(mapping)\n",
    "\n",
    "def visualize(temp):\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0,len(reverse_mapping)):\n",
    "        r[temp==l]=reverse_mapping[l][0]\n",
    "        g[temp==l]=reverse_mapping[l][1]\n",
    "        b[temp==l]=reverse_mapping[l][2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[1], temp.shape[2],3))\n",
    "\n",
    "    rgb[:,:,0] = (r)\n",
    "    rgb[:,:,1] = (g)\n",
    "    rgb[:,:,2] = (b)\n",
    "    return rgb\n",
    "\n",
    "def predict_image(model, image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    model.eval()\n",
    "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "    image = t(image)\n",
    "    model.to(device); image=image.to(device)\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0)\n",
    "        \n",
    "        start = time.time()\n",
    "        output = model(image)\n",
    "        proc_time = '{:.2f}'.format(time.time() - start)\n",
    "        print(f'推論処理時間：{proc_time}秒')\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked\n",
    "\n",
    "t_test = A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wB6IIvLYaHdP"
   },
   "source": [
    "### １－５．PyTorchの学習済みモデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIpDOwZETrQw"
   },
   "outputs": [],
   "source": [
    "model = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=24, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUqkKhzvaPB4"
   },
   "source": [
    "### １－６．モデルの推論実行（PyTorch）\n",
    "一枚の画像を推論するのにどのくらい時間がかかるか確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqkKQ-qnTxU9"
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join('DJI_0024.JPG')\n",
    "img = cv2.imread(filepath)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "aug = t_test(image=img)\n",
    "img = Image.fromarray(aug['image'])\n",
    "\n",
    "pred_mask = predict_image(model, img)\n",
    "pred_mask = pred_mask.cpu().numpy().copy()\n",
    "pred_mask = pred_mask.reshape(1,pred_mask.shape[0],pred_mask.shape[1])\n",
    "\n",
    "print(filepath)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,16))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(visualize(pred_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXDutXjPgtWz"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFnQSqjuV44T"
   },
   "source": [
    "## ２．ここからモデル最適化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnraAkVaWMmU"
   },
   "source": [
    "### ２－１．学習済みモデル（.pthファイル）をONNX形式に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMeOk5pQWI8f"
   },
   "outputs": [],
   "source": [
    "def Convert_ONNX(trained_model): \n",
    "\n",
    "    # set the model to inference mode \n",
    "    trained_model.eval() \n",
    "\n",
    "    # Let's create a dummy input tensor  \n",
    "    dummy_input = torch.randn(1, 3, 704, 1056, requires_grad=True) \n",
    "\n",
    "    # Export the model   \n",
    "    torch.onnx.export(trained_model,         # model being run \n",
    "         dummy_input,       # model input (or a tuple for multiple inputs) \n",
    "         \"Drone_Segmentation_Custum.onnx\",       # where to save the model  \n",
    "         export_params=True,  # store the trained parameter weights inside the model file \n",
    "         opset_version=11,    # the ONNX version to export the model to \n",
    "         do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "         input_names = ['modelInput'],   # the model's input names \n",
    "         output_names = ['modelOutput'], # the model's output names \n",
    "         dynamic_axes={'modelInput' : {0 : 'batch_size'},    # variable length axes \n",
    "                                'modelOutput' : {0 : 'batch_size'}}) \n",
    "    print(\" \") \n",
    "    print('Model has been converted to ONNX') \n",
    "\n",
    "Convert_ONNX(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsA0Ma_fWb6x"
   },
   "source": [
    "### ２－２．OpenVINO Model Opptimizerを用いてモデルをCPUに最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKrl4BhJWafo"
   },
   "outputs": [],
   "source": [
    "!mo --input_model Drone_Segmentation_Custum.onnx --input_shape [1,3,704,1056]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk-xHD5dWrUR"
   },
   "source": [
    "### ２－３．必要なライブラリーをインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsApe-qLWm3t"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from openvino.runtime import Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0RIo17OWv7g"
   },
   "source": [
    "### ２－４．OpenVINOのエンジンを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCzEfgmZWxY-"
   },
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "\n",
    "model = ie.read_model(model=\"Drone_Segmentation_Custum.xml\")\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "input_layer_ir = compiled_model.input(0)\n",
    "output_layer_ir = compiled_model.output(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_6Nu26aW3u4"
   },
   "source": [
    "### ２－５．OpenVINOのエンジン上で最適化モデルを実行\n",
    "最適化後は推論にどのくらいかかるか確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BS1l8A4KW9u9"
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join('DJI_0024.JPG')\n",
    "image = cv2.imread(filepath)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "aug = t_test(image=image)\n",
    "image = Image.fromarray(aug['image'])\n",
    "original_image = image\n",
    "\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "image = t(image)\n",
    "image = image.to('cpu').detach().numpy().copy()\n",
    "input_image = np.expand_dims(\n",
    "    image, 0\n",
    ")\n",
    "\n",
    "# Run the inference.\n",
    "start = time.time()\n",
    "output = compiled_model([input_image])[output_layer_ir]\n",
    "proc_time = '{:.2f}'.format(time.time() - start)\n",
    "print(f'推論処理時間：{proc_time}秒')\n",
    "\n",
    "# Prepare data for visualization.\n",
    "segmentation_mask = np.argmax(output, axis=1)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(original_image)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(visualize(segmentation_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otxsCI6Qgq7S"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejk4wOWRdqfV"
   },
   "source": [
    "## ３．ここからモデル量子化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZH0MgJoggS2"
   },
   "source": [
    "### ３－１．モデルの量子化を実行\n",
    "最もシンプルな方法で量子化を実施します。設定がシンプルな分、量子化後の精度は保証されません。\n",
    "\n",
    "\"simplified_mode_template.json\"を開き、必要であれば下記の項目をそれぞれ実際の値で上書き下さい。（本ノートブックをそのまま実行するうえでは特にデフォルト設定から変更は必要ありません。）\n",
    "* MODEL_PATH ⇒ .xml path\n",
    "* PATH_TO_WEIGHTS ⇒ .bin path\n",
    "* PATH_TO_SOURCE ⇒ 画像データフォルダーパス\n",
    "\n",
    "最後に以下のコマンドを実行すると量子化が開始します。\n",
    "（かなり時間が掛かることがあります）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7J4OWwsXKO7"
   },
   "outputs": [],
   "source": [
    "!pot -c simplified_mode_template.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr9zPGgD4E3s"
   },
   "source": [
    "---\n",
    "### 【注意】このセルは本講義のための特別処理です。\n",
    "モデルの量子化はそれなりに時間が掛かるため、量子化済みモデルを使ってこの後の処理を進めていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMskDAuvjnMV"
   },
   "outputs": [],
   "source": [
    "!curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1jq1GmHtlh6Cpo3MtSDWC2L9lwydrhEEt\" > /dev/null\n",
    "!CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n",
    "!curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1jq1GmHtlh6Cpo3MtSDWC2L9lwydrhEEt\" -o quantized_model.zip\n",
    "!unzip -qo quantized_model.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E17g-ISkIwi"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZwUAEzOf3DW"
   },
   "source": [
    "### ３－２．OpenVINOのエンジンを作成\n",
    "results/model_name_DefaultQuantization/YYYY-MM-DD_HH-MM-SS/optimized/model_name.xml\n",
    "というファイルができていると思うので、それを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WqZWQLOdUqP"
   },
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "\n",
    "model = ie.read_model(model=\"model_name.xml\")\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "input_layer_ir = compiled_model.input(0)\n",
    "output_layer_ir = compiled_model.output(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmWQEBnKf8f_"
   },
   "source": [
    "### ３－３．OpenVINOのエンジン上で最適化モデルを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nalVmz9yd9tu"
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join('DJI_0024.JPG')\n",
    "image = cv2.imread(filepath)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "aug = t_test(image=image)\n",
    "image = Image.fromarray(aug['image'])\n",
    "original_image = image\n",
    "\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "image = t(image)\n",
    "image = image.to('cpu').detach().numpy().copy()\n",
    "input_image = np.expand_dims(\n",
    "    image, 0\n",
    ")\n",
    "\n",
    "# Run the inference.\n",
    "start = time.time()\n",
    "output = compiled_model([input_image])[output_layer_ir]\n",
    "proc_time = '{:.2f}'.format(time.time() - start)\n",
    "print(f'推論処理時間：{proc_time}秒')\n",
    "\n",
    "# Prepare data for visualization.\n",
    "segmentation_mask = np.argmax(output, axis=1)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(original_image)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(visualize(segmentation_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJx6rLtugOm6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPa8KHo2VDPSlaUYa/Eil7n",
   "mount_file_id": "1JyPbhapBoXzgEWiAJoZtxtciIjy-VD-z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
